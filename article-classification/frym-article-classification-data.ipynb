{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mwS6x9O0vkz"
      },
      "source": [
        "# Frontiers for Young Minds  – Article Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U2hij2m0vk5"
      },
      "source": [
        "## Article Link Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M_PtTKT0vk6"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "sections = ['astronomy-and-space-science', \n",
        "            'biodiversity', \n",
        "            'earth-and-its-resources', \n",
        "            'human-health', \n",
        "            'mathematics', \n",
        "            'neuroscience-and-psychology'\n",
        "]\n",
        "   \n",
        "for sec in sections:\n",
        "    page = sec + '.htm'\n",
        "    fname = sec + '.txt'\n",
        "\n",
        "    html = open('webpages/' + page, encoding='utf8')\n",
        "    bs = BeautifulSoup(html)\n",
        "    \n",
        "    links = []\n",
        "    for link in bs.find_all('a', class_=\"article-link\"):\n",
        "        links.append(link.get('href'))\n",
        "    print(sec, len(links))\n",
        "    \n",
        "    with open('sections/' + fname, 'w') as f:\n",
        "        for link in links:\n",
        "            f.write(\"%s\\n\" % link)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WasZHdLK0vk-"
      },
      "source": [
        "## Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyjBV01B0vlA"
      },
      "outputs": [],
      "source": [
        "import os, pathlib\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "sections = ['astronomy-and-space-science', \n",
        "            'biodiversity', \n",
        "            'earth-and-its-resources', \n",
        "            'human-health', \n",
        "            'mathematics', \n",
        "            'neuroscience-and-psychology'\n",
        "]\n",
        "        \n",
        "base_dir = pathlib.Path('sections')\n",
        "for sec in sections:\n",
        "    sec_dir = base_dir / sec\n",
        "    os.makedirs(sec_dir)\n",
        "    \n",
        "    links = []\n",
        "    with open('sections/' + sec + '.txt') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            link = line.rstrip(\"\\n\")\n",
        "            links.append(link)\n",
        "\n",
        "    for link in links:\n",
        "        fname = link.partition('frym.')[2] + '.txt'\n",
        "        html = urlopen(link)\n",
        "        bs = BeautifulSoup(html)\n",
        "        title = bs.find('h1', class_=\"heading fulltext-heading\").text\n",
        "        paras = bs.find_all('p', id=False, class_=False)\n",
        "\n",
        "        paras_list = [title]\n",
        "        for para in paras:\n",
        "            if para.text is not None:\n",
        "                if 'conflict of interest' in para.text:\n",
        "                    break\n",
        "                else:\n",
        "                    paras_list.append(para.text)\n",
        "                    \n",
        "        with open('sections/' + sec + '/' + fname, 'w', encoding='utf-8') as f:\n",
        "            for para in paras_list:\n",
        "                f.write(\"%s\\n\" % para)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN-Cfp850vlC"
      },
      "source": [
        "## Text Standardization ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy05xRZX0vlC"
      },
      "source": [
        "#### Text standardization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a_L4Tle0vlD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def standardize(fname):\n",
        "    with open(fname, encoding='utf-8') as f:\n",
        "        paras = f.readlines()\n",
        "        paras = paras[2:]\n",
        "\n",
        "        std_lines = []\n",
        "        for para in paras:\n",
        "            para = para.rstrip(\"\\n\")\n",
        "            lines = para.split(\".\")\n",
        "            for line in lines:\n",
        "                std_line = re.sub(\"\\[\\d.*\\]\", \"\", line)\n",
        "                std_line = re.sub(\"\\(Figure.*\\)\", \"\", std_line)\n",
        "                std_lines.append(std_line)\n",
        "\n",
        "        text = str(std_lines)\n",
        "        text = text.lower()\n",
        "        text = \"\".join(char for char in text if char not in string.punctuation)\n",
        "        text = text.replace(\"’s\", \"\")\n",
        "        text = text.replace(\"’\", \"\")\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CHn0h7M0vlE"
      },
      "source": [
        "#### Text tokenization function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBCOpI2Y0vlF"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if not t.isdigit()]\n",
        "    \n",
        "    sws = stopwords.words('english')\n",
        "    tokens = [t for t in tokens if t not in sws]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14dCG0r40vlG"
      },
      "source": [
        "### Text standardization of text files from the following 4 sections:\n",
        " * **biodiversity**\n",
        " * **earth and its resources**\n",
        " * **human health**\n",
        " * **neuroscience and psychology**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHuqwxDv0vlH"
      },
      "outputs": [],
      "source": [
        "import os, pathlib\n",
        "\n",
        "secs_dir = pathlib.Path('sections')\n",
        "frym_dir = pathlib.Path('frym')\n",
        "os.makedirs(frym_dir)\n",
        "\n",
        "sections = ['biodiversity', \n",
        "            'earth-and-its-resources', \n",
        "            'human-health', \n",
        "            'neuroscience-and-psychology'\n",
        "]\n",
        "\n",
        "for sec in sections:\n",
        "        os.makedirs(frym_dir / sec)\n",
        "        \n",
        "        fnames = os.listdir(secs_dir / sec)\n",
        "        for fname in fnames:\n",
        "            fpath = 'sections/' + sec + '/'+ fname\n",
        "            text = standardize(fpath)\n",
        "            tokens = tokenize(text)\n",
        "            text = \" \".join(t for t in tokens)\n",
        "            \n",
        "            with open('frym/' + sec + '/' + fname, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"%s\" % text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "frym_article-classification_data.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}